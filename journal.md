################################################
Aug-2019; ASuMa

The current repo is an attempt to test the ULL pipeline, and some of its components,
on a number of grammars.

grammar_generator.py creates a random grammar given a set of parameters.
We will experiment with randomly created grammars, which follow a number of parameters,
specified in the header of file of such a grammar:
% RANDOM GRAMMAR with parameters:
% num_words = 20
% num_classes = 4
% num_class_connectors = 7
% connectors_limit = 2

sentence_generator.py generates random sentences using a specified grammar.

corpus_generator.py uses the previously mentioned files to generate a random corpus.

First we use very simple hand-coded grammars as a sanity test to make sure the ULL grammar
learner (https://github.com/singnet/language-learning)
does what is required in a basic POC, perhaps similar to POC-Turtle and POC-English,
but starting from a given grammar instead of arbitrary sentences. 
An advantage of generating a grammar this way is that we can get an 
arbitrary number of sentences from it, so building large lexical
corpora from it is much easier than crafting them by hand.
Equally important, we also get a Gold Standard to evaluate the processing methods.

#############################################

The first tested gramar is in data/rangram0.grammar and consists of 6 grammatical classes
(2 related to nouns, 1 to verbs, 1 to adjectives, 1 to adverbs and 1 to determiners).
Each class has only one possible word and disjuncts are created having in mind the
English language rules; no Zipfian distributions are expected in the resulting corpus.

First corpora generated in workdir are of size 5, 10, 50, 100, 200, 500 and 1000 sentences.
For each corpus, only a subset of the sentences are unique. The following lists the nbr
of unique sentences per corpus size in this experiment:

SIZE  	UNIQUE SENTS
5		5
10		10
50		28
100		48
200		68
500		75
1000	77

################################################
################################################
Sept-2019 ASuMa

Grammar Learner (GL) tests with simple rangram0.grammar and ILE were successful: grammar was learned perfectly (as evaluated by parse-evaluator F1 score) from the 5-sentence corpus. First proof of concept.
Running grammar-learner + parse-evaluator as
> language-learning/pipeline/ppln.py rangram0_5s_ILE.json
Results in 
> langauge-learning/output/rangram0/5sent/

##################
GL ILE test with 77-sentence rangram0 corpus (77 unique sentences possible, obtained from 1000 sentence rangram corpus generator):

Running grammar-learner + parse-evaluator as
> language-learning/pipeline/ppln.py rangram0_77s_ILE.json
Results in 
> langauge-learning/output/rangram0/77s_ILE/

F1 score = 95.17%
The errors are all coming from the association of "red" and "the" with both "kids" and "turtle", which is probably a bad design of this grammar, and also the grammar generator doesn't create a perfect corpus for this. It creates parses like:

the kids eat turtles
1 the 4 turtles
2 kids 3 eat
3 eat 4 turtles

which are correct according to the grammar, but feel weird in English.

######################

GL ILE test with 77-sentence rangram0 corpus:

Running grammar-learner + parse-evaluator as
> language-learning/pipeline/ppln.py rangram0_77s_ALE.json
Results in 
> langauge-learning/output/rangram0/77s_ALE/

F1 score = 97.4%

################################################
################################################
Oct-2019 ASuMa

The corpora generated by rangram can be processed with 5 different frameworks, all of which
can be evaluated by SingNet's parse-evaluator:

- Stream-parser (SP): https://github.com/glicerico/stream-parser
- OpenCog/SingNet's ULL pipeline parser (ULLP): https://github.com/singnet/learn/
- SingNet's Grammar Learner (GL): https://github.com/singnet/language-learning/
- SP + GL: Grammar learned from parses by SP
- ULLP + GL: Grammar learned from parses by ULLP

For the SP, we currently explore two parameters: the size of the observation window for pair-counting (winObserve) and the size of the parsing window for parsing (winParse).
Currently, there is no implemented weighting related with distance for word-pairs counting.

By using the [stream_evaluate](https://github.com/glicerico/stream-parser/blob/master/src/scripts/stream_evaluate.sh) script, we evaluate the complete rangram0 (77 sentences) with SP, using a range of values for winObserve and winParse. 
The parse evaluation is done against the gold standard (GS) that rangram provides for the corpus.
The following colormap summarizes F1 scores: 
![F1 scores for rangram0 processed with SP](results/plots/rangram0_77s_f1score.png)

The scores obtained range from 38.12% to 56.27%, the higher ones corresponding to the lowest values of both parameters (highest value is for `winObserve=1`, `winParse=1`)
It's interesting to note that `winObserve=1` represents sequential parses, but given that the stream-parser only links words with `PMI>0`, the sequential parses may not be covering the complete sentence.

As a baseline for comparison, a full sequential parse scores 58.42%, while the average score for random parsing this corpus is 46.6%.
Stream-parser gets close to the sequential baseline, but cannot improve it.
This is expected, seeing that the highest score is coming from incomplete sequential parses.

Most importantly, this should be seen as an exercise to test the parsing and evaluation pipelines, not as an evaluation of their capabilities.
This comes from the fact that such a limited grammar can hardly provide meaningful mutual information measures that could be leveraged by the parses that rely on these.
