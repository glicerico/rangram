# Notes on processing rangram grammars and corpora

## Aug-2019; ASuMa

The current repo is an attempt to test different components of the ULL pipeline on a number of grammars.

The `src` folder contains the code used to generate random grammars, as well as corpora from them.
This folder includes:

+ grammar_generator.py creates a random grammar given a set of parameters.

We will experiment with randomly created grammars, which follow a number of parameters,
specified in the header of file of such a grammar:
```
% RANDOM GRAMMAR with parameters:
% num_words = 20
% num_classes = 4
% num_class_connectors = 7
% connectors_limit = 2
```

+ sentence_generator.py generates random sentences using a specified grammar.

+ corpus_generator.py uses the previously mentioned files to generate a random corpus.

First we use very simple hand-coded grammars as a sanity test to make sure the [ULL grammar
learner](https://github.com/singnet/language-learning) does what is required in a basic POC, perhaps similar to POC-Turtle and POC-English, but starting from a given grammar instead of arbitrary sentences. 
An advantage of generating a grammar this way is that we can get an arbitrary number of sentences from it, so building large lexical corpora from it is much easier than crafting them by hand.
Equally important, we also get a Gold Standard to evaluate the processing methods.

***************************

The first tested gramar is in data/rangram0.grammar and consists of 6 grammatical classes
(2 related to nouns, 1 to verbs, 1 to adjectives, 1 to adverbs and 1 to determiners).
Each class has only one possible word and disjuncts are created having in mind the
English language rules; no Zipfian distributions are expected in the resulting corpus.

First corpora generated in workdir are of size 5, 10, 50, 100, 200, 500 and 1000 sentences.
For each corpus, only a subset of the sentences are unique. The following lists the nbr
of unique sentences per corpus size in this experiment:

```
SIZE  	UNIQUE SENTS
5		5
10		10
50		28
100		48
200		68
500		75
1000		77
```

****************************
## Sept-2019; ASuMa

Grammar Learner (GL) tests with simple rangram0.grammar and ILE were successful: grammar was learned perfectly (as evaluated by parse-evaluator F1 score) from the 5-sentence corpus. First proof of concept.
Running grammar-learner + parse-evaluator as
> language-learning/pipeline/ppln.py rangram0_5s_ILE.json

Results in 
> langauge-learning/output/rangram0/5sent/

***************************

GL ILE test with 77-sentence rangram0 corpus (77 unique sentences possible, obtained from 1000 sentence rangram corpus generator):

Running grammar-learner + parse-evaluator as
> language-learning/pipeline/ppln.py rangram0_77s_ILE.json

Results in 
> langauge-learning/output/rangram0/77s_ILE/

F1 score = 95.17%

The errors are all coming from the association of "red" and "the" with both "kids" and "turtle", which is probably a bad design of this grammar, and also the grammar generator doesn't create a perfect corpus for this. It creates parses like:

```
the kids eat turtles
1 the 4 turtles
2 kids 3 eat
3 eat 4 turtles
```

which are correct according to the grammar, but feel weird in English.

***************************

GL ILE test with 77-sentence rangram0 corpus:

Running grammar-learner + parse-evaluator as
> language-learning/pipeline/ppln.py rangram0_77s_ALE.json

Results in 
> langauge-learning/output/rangram0/77s_ALE/

F1 score = 97.4%

## Oct-2019; ASuMa

The corpora generated by rangram can be processed with 5 different frameworks, all of which
can be evaluated by SingNet's parse-evaluator:

- Stream-parser (SP): https://github.com/glicerico/stream-parser
- OpenCog/SingNet's ULL pipeline parser (ULLP): https://github.com/singnet/learn/
- SingNet's Grammar Learner (GL): https://github.com/singnet/language-learning/
- SP + GL: Grammar learned from parses by SP
- ULLP + GL: Grammar learned from parses by ULLP

We use two baselines as a reference:
- random parses: a random planar tree that connects all words in the sentence (no crossings, no loops).
- sequential parses: each word in a sentence is simply connected to the next one.

For the SP, we currently explore two parameters: the size of the observation window for pair-counting (winObserve) and the size of the parsing window for parsing (winParse).
Currently, there is no implemented weighting related with distance for word-pairs counting.
By using the [stream_evaluate](https://github.com/glicerico/stream-parser/blob/master/src/scripts/stream_evaluate.sh) script, we can use a range of values for winObserve and winParse easily.

Guided by experiments performed with the [ULLP](https://docs.google.com/spreadsheets/d/1TPbtGrqZ7saUHhOIi5yYmQ9c-cvVlAGqY14ATMPVCq4/edit#gid=963717716), we focus our attention to its window-based method (win=6) with distance weight for calculating PMI, and no distance-weight when MST-parsing.

For the GL, we experiment with both the ILE and ALE methods. The bulk of the parameters can be found in the respective json files of the experiments.

***************************

We first evaluate the complete rangram0 (77 sentences) with SP for winObserve and winParse up to 10.
The parse evaluation is done against the gold standard (GS) that rangram provides for the corpus.
The following colormap summarizes F1 scores: 
![F1 scores for rangram0 processed with SP](results/plots/rangram0_77s_f1score.png)

The scores obtained range from 38.12% to 56.27%, the higher ones corresponding to the lowest values of both parameters (highest value is for `winObserve=1`, `winParse=1`)
It's interesting to note that `winObserve=1` represents sequential parses, but given that the stream-parser only links words with `PMI>0`, the sequential parses may not be covering the complete sentence.

As a baseline for comparison, a full sequential parse scores 58.42%, while the average score for random parsing this corpus is 46.6%.
Stream-parser gets close to the sequential baseline, but cannot improve it.
This is expected, seeing that the highest score is coming from incomplete sequential parses.

Most importantly, this should be seen as an exercise to test the parsing and evaluation pipelines, not as an evaluation of their capabilities.
This comes from the fact that such a limited grammar can hardly provide meaningful mutual information measures that could be leveraged by the parses that rely on these.

***************************

Processing rangram0 with the ULLP, we obtain the following f1-scores:
- win6-odist		54.54%
- LG-any			54.34%
- LG-24				56.31%
- win6-omdist(4321)	54.54%

***************************

Feeding the best results of SP to GL, we get:

- best SP + GL ALE: 56.92%
- best SP + GL ILE: 56.33%

In both cases, F1 score was slightly better than its input (56.27%)

***************************

Finally, feeding the best ULLP result to GL, we get:

- best ULLP + GL ALE: 54.4%
- best ULLP + GL ILE: 55.2%

In this cases, F1 is slightly worse than its input (56.31%).

***************************

In summary, the best result for each method are shown in the following table:

F1 score [%]

|        | SP  |ULLP |GL   |SP+GL|ULLP+GL|
|--------|-----|-----|-----|-----|-------|
|rangram0|56.27|56.31|97.4 |56.92|55.2   |

************************************
